= EFK
ElasticSearch/FileBeat/Kibana
:sectnums:
:chapter-signifier: 章节
:scripts: cjk
:toc: left
:toc-title: 目录
:toclevels: 3
:doctype: book
:experimental:

== ElasticSearch 与 Kibana
=== https://www.elastic.co/guide/en/elasticsearch/reference/8.14/docker.html#docker-prod-prerequisites[官方参考^]

=== 在Swarm Manager上进行
==== 创建目录
[source,bash]
----
mkdir -p /usr/local/elasticsearch/
chmod 775 -R /usr/local/elasticsearch/
----

==== 利用临时容器初始化所需文件
. 创建并运行临时容器
+
[source,bash]
----
docker run --rm -it --net=rebue docker.elastic.co/elasticsearch/elasticsearch:8.14.2 bash
----
. 进入容器内初始化
+
https://github.com/elastic/elasticsearch/blob/8.14/docs/reference/setup/install/docker/docker compose.yml[官方参考^]
+

[source,bash]
----
# 安装ik中文分词插件
bin/elasticsearch-plugin install https://get.infini.cloud/elasticsearch/analysis-ik/8.14.2
# 安装pinyin插件
bin/elasticsearch-plugin install https://get.infini.cloud/elasticsearch/analysis-pinyin/8.14.2

# 生成CA证书并解压
bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
unzip config/certs/ca.zip -d config/certs;
# 生成数字证书
echo -ne \
          "instances:\n"\
          "  - name: es01\n"\
          "    dns:\n"\
          "      - es01\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: es02\n"\
          "    dns:\n"\
          "      - es02\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: es03\n"\
          "    dns:\n"\
          "      - es03\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          > config/certs/instances.yml;
bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
unzip config/certs/certs.zip -d config/certs;
# 修改数字证书的访问权限
#chown -R root:root config/certs;
#find . -type d -exec chmod 750 \{\} \;;
#find . -type f -exec chmod 640 \{\} \;;

----

. 将config和plugins目录复制到宿主机上(在宿主机上进行)
+
[source,bash]
----
docker cp <容器ID>:/usr/share/elasticsearch/config/ /usr/local/elasticsearch/
docker cp <容器ID>:/usr/share/elasticsearch/plugins/ /usr/local/elasticsearch/
----

=== 在所有ES宿主机上进行
==== 同步目录
同步 Swarm Manager 的 /usr/local/elasticsearch/ 目录到每台ES宿主机

==== 创建目录并配置目录可读写
ElasticSearch 在容器中运行的用户是 `elasticsearch`，`uid:gid` 是 `1000:0`
https://www.elastic.co/guide/en/elasticsearch/reference/8.14/docker.html#_configuration_files_must_be_readable_by_the_elasticsearch_user[官方参考^]

[source,bash]
----
mkdir -p /var/lib/elasticsearch/data
mkdir -p /var/lib/kibana/data/
mkdir -p /var/log/elasticsearch/
chmod g+rwx -R /usr/local/elasticsearch/config
chmod g+rwx -R /usr/local/elasticsearch/plugins
chmod g+rwx -R /var/lib/elasticsearch/data
chmod g+rwx -R /var/lib/kibana/data
chmod g+rwx -R /var/log/elasticsearch/
chgrp 0 -R /usr/local/elasticsearch/config
chgrp 0 -R /usr/local/elasticsearch/plugins
chgrp 0 -R /var/lib/elasticsearch/data
chgrp 0 -R /var/lib/kibana/data
chgrp 0 -R /var/log/elasticsearch/
----

==== 设置 `vm.max_map_count` 不小于 `262144`
* 临时生效
+
[source,bash]
----
sysctl -w vm.max_map_count=262144
----
* 永久生效
+
./etc/sysctl.conf
[source,bash]
----
....
vm.max_map_count=262144
....
----

==== 禁用交换
https://www.elastic.co/guide/en/elasticsearch/reference/8.14/docker.html#_disable_swapping[官方参考^]

* 临时生效
+
[source,bash]
----
swapoff -a
----
* 永久生效
+
./etc/sysctl.conf
[source,bash]
----
....
# 禁用内存交换
vm.swappiness=1
....
----

=== 在所有Kibana宿主机上进行

==== 同步目录
同步 Swarm Manager 的 /usr/local/elasticsearch/ 目录到每台Kibana宿主机

==== 创建目录并配置目录可读写
ElasticSearch 在容器中运行的用户是 `elasticsearch`，`uid:gid` 是 `1000:0`
https://www.elastic.co/guide/en/elasticsearch/reference/8.14/docker.html#_configuration_files_must_be_readable_by_the_elasticsearch_user[官方参考^]

[source,bash]
----
mkdir -p /var/lib/kibana/data/
chmod 775 -R /var/lib/kibana/data/
----

=== 部署(在Swarm Manager上进行)
. https://github.com/elastic/elasticsearch/blob/8.14/docs/reference/setup/install/docker/.env[.env官方参考^]
. https://github.com/elastic/elasticsearch/blob/8.14/docs/reference/setup/install/docker/docker compose.yml[docker compose.yml官方参考^]
. 准备部署的环境变量文件
+

./usr/local/elasticsearch/.env
[source,ini]
----
# network name
NETWORK_NAME=rebue

# kibana node name
KIBANA_NODE_NAME=kibana

# Password for the 'elastic' user (at least 6 characters)
ELASTIC_PASSWORD=xxxxxxxx

# Password for the 'kibana_system' user (at least 6 characters)
KIBANA_PASSWORD=xxxxxxxx

# Version of Elastic products
STACK_VERSION=8.14.2

# Set the cluster name
CLUSTER_NAME=es

# Set to 'basic' or 'trial' to automatically start the 30-day trial
LICENSE=basic
#LICENSE=trial

# Port to expose Elasticsearch HTTP API to the host
ES_PORT=9200
#ES_PORT=127.0.0.1:9200

# Port to expose Kibana to the host
KIBANA_PORT=5601
#KIBANA_PORT=80

# Increase or decrease based on the available host memory (in bytes)
MEM_LIMIT=1G

# Project namespace (defaults to the current folder name if not set)
#COMPOSE_PROJECT_NAME=myproject
----
. 准备部署配置的模板文件
+
./usr/local/elasticsearch/stack.yml.tpl
[source,yaml]
----
version: "3.9"
services:
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    hostname: es01
    volumes:
      - /usr/local/elasticsearch/config/:/usr/share/elasticsearch/config/
      - /usr/local/elasticsearch/plugins/:/usr/share/elasticsearch/plugins/
      - /var/lib/elasticsearch/data/:/usr/share/elasticsearch/data/
      - /var/log/elasticsearch/:/usr/share/elasticsearch/logs/
    environment:
      - node.name=es01
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es02,es03
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es01/es01.key
      - xpack.security.http.ssl.certificate=certs/es01/es01.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es01/es01.key
      - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    #  - ES_JAVA_OPTS="-Xms1g -Xmx1g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65535
        hard: 65535
    deploy:
      resources:
        limits:
          memory: ${MEM_LIMIT}
      placement:
        constraints:
          - node.hostname==es01
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    logging:
      options:
        max-size: 8m

  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    hostname: es02
    volumes:
      - /usr/local/elasticsearch/config/:/usr/share/elasticsearch/config/
      - /usr/local/elasticsearch/plugins/:/usr/share/elasticsearch/plugins/
      - /var/lib/elasticsearch/data/:/usr/share/elasticsearch/data/
      - /var/log/elasticsearch/:/usr/share/elasticsearch/logs/
    environment:
      - node.name=es02
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es03
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es02/es02.key
      - xpack.security.http.ssl.certificate=certs/es02/es02.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es02/es02.key
      - xpack.security.transport.ssl.certificate=certs/es02/es02.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    #  - ES_JAVA_OPTS="-Xms1g -Xmx1g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    deploy:
      resources:
        limits:
          memory: ${MEM_LIMIT}
      placement:
        constraints:
          - node.hostname==es02
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    logging:
      options:
        max-size: 8m

  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    hostname: es03
    volumes:
      - /usr/local/elasticsearch/config/:/usr/share/elasticsearch/config/
      - /usr/local/elasticsearch/plugins/:/usr/share/elasticsearch/plugins/
      - /var/lib/elasticsearch/data/:/usr/share/elasticsearch/data/
      - /var/log/elasticsearch/:/usr/share/elasticsearch/logs/
    environment:
      - node.name=es03
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es02
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es03/es03.key
      - xpack.security.http.ssl.certificate=certs/es03/es03.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es03/es03.key
      - xpack.security.transport.ssl.certificate=certs/es03/es03.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    #  - ES_JAVA_OPTS="-Xms1g -Xmx1g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    deploy:
      resources:
        limits:
          memory: ${MEM_LIMIT}
      placement:
        constraints:
          - node.hostname==es03
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    logging:
      options:
        max-size: 8m

  kibana:
    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
    volumes:
      - /usr/local/elasticsearch/config/certs/:/usr/share/kibana/config/certs
      - /var/lib/kibana/data/:/usr/share/kibana/data
    #ports:
    #  - ${KIBANA_PORT}:5601
    environment:
      - SERVERNAME=kibana
      - ELASTICSEARCH_HOSTS=["https://es01:9200","https://es02:9200","https://es03:9200"]
      - SERVER_BASEPATH=/kibana
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
    deploy:
      placement:
        constraints:
          - node.hostname==${KIBANA_NODE_NAME}
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    logging:
      options:
        max-size: 8m

networks:
  default:
    external: true
    name: ${NETWORK_NAME}
----

. 通过模板生成部署配置文件
** envsubst方式
.. 安装 envsubst
+
[source,bash]
----
yum install -y gettext
----
.. 生成部署配置文件
+
[source,bash]
----
# 读取环境变量
source /usr/local/elasticsearch/.env
# 注入部署配置文件
envsubst < /usr/local/elasticsearch/stack.yml.tpl > /usr/local/elasticsearch/stack.yml
----
** envsubst.py方式
.. https://www.cnblogs.com/leoninew/p/13516223.html[参考^]
.. 代码
+
[source,python]
----
include::envsubst.py[]
----
.. 生成部署配置文件
+
[source,bash]
----
python3 /usr/local/elasticsearch/envsubst.py --env-file /usr/local/elasticsearch/.env -f /usr/local/elasticsearch/stack.yml.tpl > /usr/local/elasticsearch/stack.yml
----

. 部署
+
[source,bash]
----
docker stack deploy -c /usr/local/elasticsearch/stack.yml es
----

=== 在ES中添加kibana访问的账户和密码(在临时容器中进行)
. 创建并运行临时容器
+
[source,bash]
----
docker run --rm -it --net=rebue --env-file /usr/local/elasticsearch/.env \
-v /usr/local/elasticsearch/config/certs/:/usr/share/elasticsearch/config/certs/ \
docker.elastic.co/elasticsearch/elasticsearch:8.14.2 bash
----
. 进入容器内添加kibana访问的账户和密码
+
https://github.com/elastic/elasticsearch/blob/8.14/docs/reference/setup/install/docker/docker-compose.yml[官方参考^]
+

[source,bash]
----
# 在ES中添加kibana访问的账户和密码(下面的执行要等es部署好后再运行)
curl -s -X POST --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" https://es01:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}"
----

== FileBeat
=== 官方参考
* https://www.elastic.co/guide/en/beats/filebeat/current/running-on-docker.html[在docker中运行FileBeat^]
* https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-reference-yml.html[配置文件^]
* https://www.elastic.co/guide/en/beats/filebeat/current/securing-communication-elasticsearch.html[与ElasticSearch安全通讯^]

=== 在ES中添加filebeat通讯所需内容(在临时容器中进行)
. 创建并运行临时容器
+
[source,bash]
----
docker run --rm -it --net=rebue --env-file /usr/local/elasticsearch/.env \
-v /usr/local/elasticsearch/config/certs/:/usr/share/elasticsearch/config/certs/ \
docker.elastic.co/elasticsearch/elasticsearch:8.14.2 bash
----
. 进入容器内执行下面内容(要等ES部署好后再运行)
+

[source,bash]
----
# 在ES中添加filebeat访问所需的API_KEY(注意修改里面的rebue-*改为实际的名称)
curl -s -X POST --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" https://es01:9200/_security/api_key -d "{\"name\":\"filebeat_system\",\"role_descriptors\":{\"filebeat_writer\":{\"cluster\":[\"monitor\",\"read_ilm\",\"read_pipeline\"],\"index\":[{\"names\":[\"filebeat-*\",\"rebue-*\"],\"privileges\":[\"view_index_metadata\",\"create_doc\",\"auto_configure\"]}]}}}"

# ....
# 记录上面返回的id和api_key，字符串格式为 `id:api_key`，后面的配置要用
# ....

# 在ES中添加名为rebue的pipeline(这里的rebue可以根据实际命名)
curl -s -X PUT --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" https://es01:9200/_ingest/pipeline/rebue -d '{	\"description\":\"rebue相关的微服务\",	\"processors\":[{		\"script\":{			\"source\":\"if(!ctx.appid?.startsWith('gatex')){\nreturn;\n}\n\ndef matcher=/\\|[0-9a-f]{8}\\|[0-9a-f\\x20]+\\|(.+)\\|/.matcher(ctx.message);\ndef mergedMatches=[];\nwhile(matcher.find()){\nif(matcher.groupCount()>0){\nmergedMatches.add(matcher.group(1));\n}\n}\nif(mergedMatches.length>0){\nctx.data=mergedMatches.join('');\n}\n\"}}]}'
----

. 如果添加pipeline不成功，直接在kibana里面添加
+
左上角 `D` -> `Manage Spaces` -> Ingest 下的 `Ingest Pipelines` -> `Create pipeline`
+
.rebue.painless
[source,painless]
----
// 此段脚本用于从netty的详细日志中解析出有用的部分重写入data字段中
if (!ctx.appid?.startsWith('gatex')) {
    return;
}
def matcher = /\|[0-9a-f]{8}\|[0-9a-f\x20]+\|(.+)\|/.matcher(ctx.message);
def mergedMatches = [];
while (matcher.find()) {
    if (matcher.groupCount() > 0) {
        mergedMatches.add(matcher.group(1));
    }
}
if (mergedMatches.length > 0) {
    ctx.data = mergedMatches.join('');
}
----

. 在 kibana 上设置索引模板保留 3 天数据
左上角 `D` -> `Manage Spaces` -> Data 下的 `Index Management` -> `Index Templates` -> 找到同名索引 -> `Manage` 下 `Edit` -> 在 `Data retention` 下启用 `Enable data retention` -> 设置保留天数

=== 创建filebeat的配置文件
./usr/local/elasticsearch/filebeat/filebeat.yml
[source,yaml]
----
filebeat.inputs:
  - type: filestream
    id: rebue
    # 是否启动
    enabled: true
    # 从那个路径收集日志，如果存在多个 input ,则这个 paths 中的收集的日志最好不要重复，否则会出现问题
    # 日志路径可以写通配符
    paths:
      - /var/log/*/current.log
    field_under_root: true
    parsers:
      # 多行日志的合并处理
      - multiline:
          pattern: ^[0-9]{4}-[0-9]{2}-[0-9]{2}
          negate: true
          match: after
    # 使用es的ignes node 的pipeline处理数据，这个理论上要配置到output.elasticsearch下方，但是测试的时候发现配置在output.elasticsearch下方不生效。
    pipeline: rebue

# 队列最小事件数(默认2048，必须小于下面的队列内存缓冲的最大事件数)
queue.mem.flush.min_events: 128
# 队列内存缓冲的最大事件数
queue.mem.events: 256
# 收割机获取文件的缓冲大小
harvester_buffer_size: 2M
# 单条日志最大字节数，单位 B，1M(默认是10M)
max_bytes: 2M

processors:
  - script:
      lang: javascript
      # 相对当前配置文件所在的路径
      file: ${path.config}/process.js
  # https://www.elastic.co/guide/en/beats/filebeat/current/processor-timestamp.html
  - timestamp:
      # 格式化时间值 给 时间戳
      field: start_time
      timezone: Asia/Shanghai
      # 这个格式内容是写死的，不能改任何数字
      layouts:
        - "2006-01-02 15:04:05"
        - "2006-01-02 15:04:05.999"
      test:
        - "2019-06-22 16:33:51"
  - drop_fields:
      # start_time字段的值已经设置到 @timpstamp 了，不用发送此字段
      fields: [start_time]

setup:
  # 自定义索引
  template:
    # enabled: true
    name: "rebue"
    pattern: "rebue-*"
  # ilm:
  #   enabled: false

output.elasticsearch:
  hosts:
    - https://es01:9200
    - https://es02:9200
    - https://es03:9200
  ssl.certificate_authorities: /usr/share/filebeat/config/certs/ca/ca.crt
  # 上一步获取的id和api_key，格式为 `id:api_key`
  api_key: xxxxxxxx
  index: shdxgc-%{[appid]}-%{+yyyy.MM.dd}
  # pipeline: rebue

----

./usr/local/elasticsearch/filebeat/script/process.js
[source,js]
----
function process(event) {
    // 从文件路径中解析出appid并保存到字段中
    var filePath = event.Get('log').file.path;
    var appIdRegex = /^\/var\/log\/([^\/]+)/;
    var appIdMatch = filePath.match(appIdRegex);
    event.Put("appid", appIdMatch[1]);

    // 解析mssage
    var message = event.Get("message");
    // Define message regex pattern
    var timestampRegex = /^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3})\[(.+?)\]([\s\S]*)---/;
    var timestampMatch = message.match(timestampRegex); // Match the timestamp in the log message
    if (timestampMatch) {
        var start_time = timestampMatch[1];
        var loglevel = timestampMatch[2];
        var data = timestampMatch[3];
        event.Put("start_time", start_time); // Set the parsed timestamp as @timestamp field
        event.Put("log.level", loglevel); // Set the parsed timestamp as @timestamp field
        event.Put("data", data); // Set the parsed timestamp as @timestamp field
    }

    return event;
}
----

./usr/local/elasticsearch/filebeat/stack.yml
[source,yaml]
----
version: "3.9"
services:
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.14.2
    user: root
    command: filebeat -e -strict.perms=false
    volumes:
      - /usr/local/elasticsearch/filebeat/config/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /usr/local/elasticsearch/filebeat/script/process.js:/usr/share/filebeat/process.js:ro
      - /usr/local/elasticsearch/config/certs/:/usr/share/filebeat/config/certs/
      - /var/log/:/var/log/
    deploy:
      mode: global
      placement:
        constraints:
          - node.labels.role!=es
          - node.labels.role!=redis
          - node.labels.role!=kafka
          - node.labels.role!=rabbitmq
    logging:
      options:
        max-size: 8m

networks:
  default:
    external: true
    name: shdxgc
----

=== 部署
[source,bash]
----
docker stack deploy -c /usr/local/elasticsearch/filebeat/stack.yml es
----

== 常见问题
=== ES 磁盘满
. 报错 `TOO_MANY_REQUESTS/12/disk usage exceeded flood-stage watermark, index has read-only-allow-delete block`
+
. 创建运行并进入临时容器
+
[source,bash]
----
docker run --rm -it --net=rebue --env-file /usr/local/elasticsearch/.env \
-v /usr/local/elasticsearch/config/certs/:/usr/share/elasticsearch/config/certs/ \
docker.elastic.co/elasticsearch/elasticsearch:8.14.2 bash
----
. 检查现有索引
+
[source,bash]
----
curl --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" "https://es01:9200/_cat/indices?v&pretty"
----
. 删除指定索引
+
[source,bash]
----
curl -X DELETE --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" "https://es01:9200/<index-name>"
----
. 查看所有数据流
+
[source,bash]
----
curl --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" "https://es01:9200/_data_stream/*?pretty"
----
. 查看指定数据流
+
[source,bash]
----
curl --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" "https://es01:9200/_data_stream/<data_stream_name>?pretty"
----
. 删除数据流
+
[source,bash]
----
curl -X DELETE --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" "https://es01:9200/_data_stream/<data_stream_name>"
----
. 关闭索引的只读状态
+
[source,bash]
----
# 在ES中添加kibana访问的账户和密码(下面的执行要等es部署好后再运行)
curl -X PUT --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" https://es01:9200/_all/_settings -d "{\"index.blocks.read_only_allow_delete\": null}"
----


== 附录
=== filebeat
https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-reference-yml.html[配置参考^]
https://www.elastic.co/guide/en/beats/filebeat/8.14/filtering-and-enhancing-data.html#filtering-and-enhancing-data[过滤与增强数据^]

=== grok
https://github.com/elastic/elasticsearch/blob/8.14/libs/grok/src/main/resources/patterns/ecs-v1/grok-patterns[Grok内置模式^]

=== painess
https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-lang-spec.html[语言^]
https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-api-reference.html[API^]
https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-api-reference-shared.html[Shared API^]
https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-regexes.html[正则^]